{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0001, 0.00012589254117941674, 0.00015848931924611142, 0.00019952623149688809, 0.00025118864315095823, 0.00031622776601683826, 0.00039810717055349773, 0.00050118723362727296, 0.00063095734448019429, 0.00079432823472428294, 0.001000000000000002, 0.0012589254117941701, 0.0015848931924611173, 0.001995262314968885, 0.0025118864315095872, 0.0031622776601683889, 0.0039810717055349856, 0.0050118723362727402, 0.0063095734448019554, 0.0079432823472428467]\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "print(regul_val)\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEOCAYAAACEiBAqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5SIv+gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff85ac03ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  layer1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(layer1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) +\\\n",
    "    beta * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  layer1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(layer1_valid, weights2) + biases2)\n",
    "  layer1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(layer1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:    \n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(accuracy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEOCAYAAACD5gx6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nghcRiSgleBGRiFKCFxGJqP8HZ6RIZcGL5lwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff80d9e3810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1-layer net)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  layer1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(layer1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) \n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  layer1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(layer1_valid, weights2) + biases2)\n",
    "  layer1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(layer1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 496.288910\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 41.9%\n",
      "Minibatch loss at step 2: 923.586609\n",
      "Minibatch accuracy: 47.7%\n",
      "Validation accuracy: 31.6%\n",
      "Minibatch loss at step 4: 555.670776\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy: 52.3%\n",
      "Minibatch loss at step 6: 80.153122\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 63.6%\n",
      "Minibatch loss at step 8: 3.580130\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.3%\n",
      "Minibatch loss at step 10: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 12: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 14: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 16: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 18: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 20: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 22: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 24: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 26: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 28: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 32: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 34: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 36: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 38: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 42: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 44: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 48: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 52: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 54: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 56: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 58: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 64: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 66: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 68: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 76: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 78: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 82: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 84: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 86: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 88: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 94: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 96: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Test accuracy: 72.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_bacthes = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_bacthes\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  layer1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop = tf.nn.dropout(layer1_train, 0.5)\n",
    "  logits = tf.matmul(drop, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) \n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  layer1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(layer1_valid, weights2) + biases2)\n",
    "  layer1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(layer1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 501.888062\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 28.9%\n",
      "Minibatch loss at step 2: 1050.822021\n",
      "Minibatch accuracy: 54.7%\n",
      "Validation accuracy: 41.1%\n",
      "Minibatch loss at step 4: 144.414764\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 6: 19.342125\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 69.6%\n",
      "Minibatch loss at step 8: 2.665114\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 70.4%\n",
      "Minibatch loss at step 10: 0.497884\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 70.5%\n",
      "Minibatch loss at step 12: 3.610359\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 70.6%\n",
      "Minibatch loss at step 14: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.4%\n",
      "Minibatch loss at step 16: 0.821183\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 70.1%\n",
      "Minibatch loss at step 18: 2.036552\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.3%\n",
      "Minibatch loss at step 20: 0.000006\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.4%\n",
      "Minibatch loss at step 22: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.1%\n",
      "Minibatch loss at step 24: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.1%\n",
      "Minibatch loss at step 26: 0.045491\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch loss at step 28: 2.945062\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 70.9%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.9%\n",
      "Minibatch loss at step 32: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.9%\n",
      "Minibatch loss at step 34: 0.572580\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.6%\n",
      "Minibatch loss at step 36: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.6%\n",
      "Minibatch loss at step 38: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.1%\n",
      "Minibatch loss at step 40: 0.573162\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch loss at step 42: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch loss at step 44: 2.623197\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 71.4%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.4%\n",
      "Minibatch loss at step 48: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.4%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.4%\n",
      "Minibatch loss at step 52: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.5%\n",
      "Minibatch loss at step 54: 0.967958\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.8%\n",
      "Minibatch loss at step 56: 0.335073\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.5%\n",
      "Minibatch loss at step 58: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.5%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.5%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.5%\n",
      "Minibatch loss at step 64: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.4%\n",
      "Minibatch loss at step 66: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.4%\n",
      "Minibatch loss at step 68: 0.323977\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.4%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch loss at step 76: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch loss at step 78: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch loss at step 82: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch loss at step 84: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch loss at step 86: 0.027012\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 71.3%\n",
      "Minibatch loss at step 88: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.3%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.0%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.8%\n",
      "Minibatch loss at step 94: 0.772364\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.8%\n",
      "Minibatch loss at step 96: 1.116491\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 71.0%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.0%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.0%\n",
      "Test accuracy: 78.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_bacthes = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_bacthes\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 512\n",
    "num_hidden_nodes3 = 256\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  layer1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(layer1_train, keep_prob)\n",
    "  layer2_train = tf.nn.relu(tf.matmul(drop1, weights2) + biases2)\n",
    "  drop2 = tf.nn.dropout(layer2_train, keep_prob)\n",
    "  layer3_train = tf.nn.relu(tf.matmul(drop2, weights3) + biases3)\n",
    "  drop3 = tf.nn.dropout(layer3_train, keep_prob)\n",
    "  logits = tf.matmul(drop3, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  layer1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  layer2_valid = tf.nn.relu(tf.matmul(layer1_valid, weights2) + biases2)\n",
    "  layer3_valid = tf.nn.relu(tf.matmul(layer2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(layer3_valid, weights4) + biases4)\n",
    "  layer1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  layer2_test = tf.nn.relu(tf.matmul(layer1_test, weights2) + biases2)\n",
    "  layer3_test = tf.nn.relu(tf.matmul(layer2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(layer3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.901313\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 28.4%\n",
      "Minibatch loss at step 500: 0.600283\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 1000: 0.423143\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 1500: 0.645969\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 2000: 0.504387\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 2500: 0.631910\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 3000: 0.499912\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 3500: 0.356062\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 4000: 0.462285\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 4500: 0.264738\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 5000: 0.258516\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 5500: 0.448653\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 6000: 0.422790\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 6500: 0.314060\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 7000: 0.753905\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 7500: 0.373288\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 8000: 0.460098\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 8500: 0.358016\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 9000: 0.387821\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 9500: 0.477308\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 10000: 0.371592\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 10500: 0.350366\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 11000: 0.415365\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 11500: 0.471212\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 12000: 0.460255\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 12500: 0.377110\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 13000: 0.341583\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 13500: 0.295173\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 14000: 0.302517\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 14500: 0.290935\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 15000: 0.211593\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 15500: 0.382615\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 16000: 0.294260\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 16500: 0.254384\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 17000: 0.427610\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 17500: 0.253846\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 18000: 0.222302\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 18500: 0.453091\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 19000: 0.265597\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 19500: 0.292284\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 20000: 0.272666\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.4%\n",
      "Test accuracy: 95.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
